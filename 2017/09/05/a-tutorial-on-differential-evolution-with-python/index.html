<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A tutorial on Differential Evolution with Python | Pablo Rodriguez Mier </title> <meta name="author" content="Pablo Rodriguez Mier"> <meta name="description" content="an example of a blog post with disqus comments"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pablormier.github.io/2017/09/05/a-tutorial-on-differential-evolution-with-python/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Pablo </span> Rodriguez  Mier </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A tutorial on Differential Evolution with Python</h1> <p class="post-meta"> September 05, 2017 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2017   ·   <i class="fa-solid fa-tag fa-sm"></i> python   <i class="fa-solid fa-tag fa-sm"></i> optimization   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I have to admit that I’m a great fan of the Differential Evolution (DE) algorithm. This algorithm, invented by <a href="https://link.springer.com/article/10.1023%2FA%3A1008202821328?LI=true" rel="external nofollow noopener" target="_blank">R. Storn and K. Price</a> in 1997, is a very powerful algorithm for black-box optimization (also called derivative-free optimization). Black-box optimization is about finding the minimum of a function \(f(x): \mathbb{R}^n \rightarrow \mathbb{R}\), where we don’t know its analytical form, and therefore no derivatives can be computed to minimize it (or are hard to approximate). The figure below shows how the DE algorithm approximates the minimum of a function in succesive steps:</p> <center> <div class="l-body"> <img src="/assets/img/de/ackley.gif"> <div class="caption"> <b>Figure 1.</b> Example of DE iteratively optimizing the 2D <a href="https://www.sfu.ca/~ssurjano/ackley.html" rel="external nofollow noopener" target="_blank">Ackley function</a> (generated using <a href="https://github.com/pablormier/yabox" rel="external nofollow noopener" target="_blank">Yabox</a>) </div> </div> </center> <p>The optimization of black-box functions is very common in real world problems, where the function to be optimized is very complex (and may involve the use of simulators or external software for the computations). For these kind of problems, DE works pretty well, and that’s why it’s very popular for solving problems in many different fields, including Astronomy, Chemistry, Biology, and many more. For example, the European Space Agency (ESA) uses DE to <a href="http://www.esa.int/gsp/ACT/doc/INF/pub/ACT-RPR-INF-2014-(PPSN)CstrsOptJupiterCapture.pdf" rel="external nofollow noopener" target="_blank">design optimal trajectories</a> in order to reach the orbit of a planet using as less fuel as possible. Sounds awesome right? Best of all, the algorithm is very simple to understand and to implement. In this tutorial, we will see how to implement it, how to use it to solve some problems and we will build intuition about how DE works.</p> <h1 id="lets-start">Let’s start!</h1> <p>Before getting into more technical details, let’s get our hands dirty. One thing that fascinates me about DE is not only its power but its simplicity, since it can be implemented in just a few lines. Here is the code for the DE algorithm using the <em>rand/1/bin</em> schema (we will talk about what this means later). It only took me 27 lines of code using Python with Numpy:</p> <script src="https://gist.github.com/pablormier/0caff10a5f76e87857b44f63757729b0.js"></script> <p>This code is completely functional, you can paste it into a python terminal and start playing with it (you need numpy &gt;= 1.7.0). Don’t worry if you don’t understand anything, we will see later what is the meaning of each line in this code. The good thing is that we can start playing with this right now without knowing how this works. The only two mandatory parameters that we need to provide are <strong>fobj</strong> and <strong>bounds</strong>:</p> <ul> <li> <p><strong>fobj</strong>: \(f(x)\) function to optimize. Can be a function defined with a <code class="language-plaintext highlighter-rouge">def</code> or a lambda expression. For example, suppose we want to minimize the function \(f(x)=\sum_i^n x_i^2/n\). If <code class="language-plaintext highlighter-rouge">x</code> is a numpy array, our fobj can be defined as:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">fobj</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div> </div> <p>If we define <code class="language-plaintext highlighter-rouge">x</code> as a list, we should define our objective function in this way:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fobj</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="n">value</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
      <span class="n">value</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
  <span class="k">return</span> <span class="n">value</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>bounds</strong>: a list with the lower and upper bound for each parameter of the function. For example: \(bounds_x=\) [(-5, 5), (-5, 5), (-5, 5), (-5, 5)] means that each variable \(x_i, i \in [1, 4]\) is bound to the interval [-5, 5].</p> </li> </ul> <p>For example, let’s find the value of x that minimizes the function \(f(x) = x^2\), looking for values of \(x\) between -100 and 100:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">it</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="n">it</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="p">(</span><span class="nf">array</span><span class="p">([</span> <span class="mf">0.</span><span class="p">]),</span> <span class="nf">array</span><span class="p">([</span> <span class="mf">0.</span><span class="p">]))</span>
</code></pre></div></div> <p>The first value returned (<code class="language-plaintext highlighter-rouge">array([ 0.]</code>) represents the best value for <code class="language-plaintext highlighter-rouge">x</code> (in this case is just a single number since the function is 1-D), and the value of <code class="language-plaintext highlighter-rouge">f(x)</code> for that <code class="language-plaintext highlighter-rouge">x</code> is returned in the second array (<code class="language-plaintext highlighter-rouge">array([ 0.]</code>).</p> <blockquote> <p>Note: for convenience, I defined the <code class="language-plaintext highlighter-rouge">de</code> function as a generator function that yields the best solution \(x\) and its corresponding value of \(f(x)\) at each iteration. In order to obtain the last solution, we only need to consume the iterator, or convert it to a list and obtain the last value with <code class="language-plaintext highlighter-rouge">list(de(...))[-1]</code></p> </blockquote> <p>Yeah I know, this is too easy. Now, let’s try the same example in a multi-dimensional setting, with the function now defined as \(f(x) = \sum_{i}^n x_i^2 / n\), for n=32 dimensions. This is how it looks like in 2D:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># https://github.com/pablormier/yabox
# pip install yabox
</span><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="n">yabox.problems</span> <span class="kn">import</span> <span class="n">problem</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">problem</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">2</span><span class="p">).</span><span class="nf">plot3d</span><span class="p">()</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/x2.png"> <div class="caption"> <b>Figure 2.</b> Representation of \(f(x)=\sum x_i^2/n\) </div> </div> </center> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">32</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="p">(</span><span class="nf">array</span><span class="p">([</span> <span class="mf">1.43231366</span><span class="p">,</span>  <span class="mf">4.83555112</span><span class="p">,</span>  <span class="mf">0.29051824</span><span class="p">,</span>  <span class="mf">2.94836318</span><span class="p">,</span>  <span class="mf">2.02918578</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">2.60703144</span><span class="p">,</span>  <span class="mf">0.76783095</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.66057484</span><span class="p">,</span>  <span class="mf">0.42346029</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.36945634</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.01227915</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.38498397</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.76757209</span><span class="p">,</span>  <span class="mf">3.16294878</span><span class="p">,</span>  <span class="mf">5.96335235</span><span class="p">,</span>
        <span class="mf">3.51911452</span><span class="p">,</span>  <span class="mf">1.24422323</span><span class="p">,</span>  <span class="mf">2.9985505</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.13640705</span><span class="p">,</span>  <span class="mf">0.47221648</span><span class="p">,</span>
        <span class="mf">0.42467349</span><span class="p">,</span>  <span class="mf">0.26045357</span><span class="p">,</span>  <span class="mf">1.20885682</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6256121</span> <span class="p">,</span>  <span class="mf">2.21449962</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.23379811</span><span class="p">,</span>  <span class="mf">2.20160374</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1396289</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.72875512</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.46034836</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">5.84519163</span><span class="p">,</span>  <span class="mf">2.66791339</span><span class="p">]),</span> <span class="mf">6.3464570348900136</span><span class="p">)</span>
</code></pre></div></div> <p>This time the best value for <code class="language-plaintext highlighter-rouge">f(x)</code> was <code class="language-plaintext highlighter-rouge">6.346</code>, we didn’t obtained the optimal solution \(f(0, \dots, 0) = 0\). Why? DE doesn’t guarantee to obtain the global minimum of a function. What it does is to approach the global minimum in successive steps, as shown in Fig. 1. So in general, the more complex the function, the more iterations are needed. This can raise a new question: how does the dimensionality of a function affects the convergence of the algorithm? In general terms, the difficulty of finding the optimal solution increases exponentially with the number of dimensions (parameters). This effect is called “<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="external nofollow noopener" target="_blank">curse of dimensionality</a>”. For example, suppose we want to find the minimum of a 2D function whose input values are binary. Since they are binary and there are only two possible values for each one, we would need to evaluate in the worst case \(2^2 = 4\) combinations of values: \(f(0,0)\), \(f(0,1)\), \(f(1,0)\) and \(f(1,1)\). But if we have 32 parameters, we would need to evaluate the function for a total of \(2^{32}\) = 4,294,967,296 possible combinations in the worst case (the size of the search space grows exponentially). This makes the problem much much more difficult, and any metaheuristic algorithm like DE would need many more iterations to find a good approximation.</p> <p>Knowing this, let’s run again the algorithm but for 3,000 iterations instead of just 1,000:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="p">[(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">32</span><span class="p">,</span> <span class="n">its</span><span class="o">=</span><span class="mi">3000</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="p">(</span><span class="nf">array</span><span class="p">([</span> <span class="mf">0.00648831</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00093694</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00205017</span><span class="p">,</span>  <span class="mf">0.00136862</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00722833</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.00138284</span><span class="p">,</span>  <span class="mf">0.00323691</span><span class="p">,</span>  <span class="mf">0.0040672</span> <span class="p">,</span> <span class="o">-</span><span class="mf">0.0060339</span> <span class="p">,</span>  <span class="mf">0.00631543</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.01132894</span><span class="p">,</span>  <span class="mf">0.00020696</span><span class="p">,</span>  <span class="mf">0.00020962</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00063984</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00877504</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.00227608</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00101973</span><span class="p">,</span>  <span class="mf">0.00087068</span><span class="p">,</span>  <span class="mf">0.00243963</span><span class="p">,</span>  <span class="mf">0.01391991</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.00894368</span><span class="p">,</span>  <span class="mf">0.00035751</span><span class="p">,</span>  <span class="mf">0.00151198</span><span class="p">,</span>  <span class="mf">0.00310393</span><span class="p">,</span>  <span class="mf">0.00219394</span><span class="p">,</span>
        <span class="mf">0.01290131</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00029911</span><span class="p">,</span>  <span class="mf">0.00343577</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.00032941</span><span class="p">,</span>  <span class="mf">0.00021377</span><span class="p">,</span>
       <span class="o">-</span><span class="mf">0.01015071</span><span class="p">,</span>  <span class="mf">0.00389961</span><span class="p">]),</span> <span class="mf">3.1645278699373536e-05</span><span class="p">)</span>
</code></pre></div></div> <p>Now we obtained a much better solution, with a value very close to 0. In this case we only needed a few thousand iterations to obtain a good approximation, but with complex functions we would need much more iterations, and yet the algorithm could get trapped in a local minimum. We can plot the convergence of the algorithm very easily (now is when the implementation using a generator function comes in handy):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">result</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/de3000.png"> <div class="caption"> <b>Figure 3.</b> Evolution of the best solution found by DE in each iteration </div> </div> </center> <p>Fig. 3 shows how the best solution found by the algorithm approximates more and more to the global minimum as more iterations are executed. Now we can represent in a single plot how the complexity of the function affects the number of iterations needed to obtain a good approximation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]:</span>
    <span class="n">it</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">d</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span> <span class="o">*</span> <span class="n">d</span><span class="p">,</span> <span class="n">its</span><span class="o">=</span><span class="mi">3000</span><span class="p">))</span>    
    <span class="n">x</span><span class="p">,</span> <span class="n">f</span> <span class="o">=</span> <span class="nf">zip</span><span class="p">(</span><span class="o">*</span><span class="n">it</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">d={}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/comp_de3000.png"> <div class="caption"> <b>Figure 4.</b> Comparison of the convergence speed for different dimensions </div> </div> </center> <p>The plot makes it clear that when the number of dimensions grows, the number of iterations required by the algorithm to find a good solution grows as well.</p> <h1 id="how-it-works">How it works?</h1> <p>Now it’s time to talk about how these 27 lines of code work. Differential Evolution, as the name suggest, is a type of evolutionary algorithm. An evolutionary algorithm is an algorithm that uses mechanisms inspired by the theory of evolution, where the fittest individuals of a population (the ones that have the traits that allow them to survive longer) are the ones that produce more offspring, which in turn inherit the good traits of the parents. This makes the new generation more likely to survive in the future as well, and so the population improves over time, generation after generation. This is possible thanks to different mechanisms present in nature, such as mutation, recombination and selection, among others. Evolutionary algorithms apply some of these principles to <em>evolve</em> a solution to a problem.</p> <p>In this way, in Differential Evolution, solutions are represented as populations of individuals (or vectors), where each individual is represented by a set of real numbers. These real numbers are the values of the parameters of the function that we want to minimize, and this function <em>measures</em> how good an individual is. The main steps of the algorithm are: initialization of the population, mutation, recombination, replacement and evaluation. Let’s see how these operations are applied working through a simple example of minimizing the function \(f(\mathbf{x})=\sum x_i^2/n\) for \(n=4\), so \(\mathbf{x}=\{x_1, x_2, x_3, x_4\}\), and \(-5 \leq x_i \leq 5\).</p> <h2 id="components">Components</h2> <ul> <li> <strong>fobj</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">fobj</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>bounds</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">4</span>
</code></pre></div> </div> </li> </ul> <h2 id="initialization">Initialization</h2> <p>The first step in every evolutionary algorithm is the creation of a population with <em>popsize</em> individuals. An individual is just an instantiation of the parameters of the function <strong>fobj</strong>. At the beginning, the algorithm initializes the individuals by generating random values for each parameter within the given bounds. For convenience, I generate uniform random numbers between 0 and 1, and then I scale the parameters (denormalization) to obtain the corresponding values. This is done in lines 4-8 of the algorithm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Population of 10 individuals, 4 params each (popsize = 10, dimensions = 4)
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">pop</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">popsize</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pop</span>

<span class="c1">#        x[0]   x[1]   x[2]   x[3]
</span><span class="nf">array</span><span class="p">([[</span> <span class="mf">0.09</span><span class="p">,</span>  <span class="mf">0.01</span><span class="p">,</span>  <span class="mf">0.4</span> <span class="p">,</span>  <span class="mf">0.21</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.04</span><span class="p">,</span>  <span class="mf">0.87</span><span class="p">,</span>  <span class="mf">0.52</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.96</span><span class="p">,</span>  <span class="mf">0.78</span><span class="p">,</span>  <span class="mf">0.65</span><span class="p">,</span>  <span class="mf">0.17</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.22</span><span class="p">,</span>  <span class="mf">0.83</span><span class="p">,</span>  <span class="mf">0.23</span><span class="p">,</span>  <span class="mf">0.84</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.8</span> <span class="p">,</span>  <span class="mf">0.43</span><span class="p">,</span>  <span class="mf">0.06</span><span class="p">,</span>  <span class="mf">0.44</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.95</span><span class="p">,</span>  <span class="mf">0.99</span><span class="p">,</span>  <span class="mf">0.93</span><span class="p">,</span>  <span class="mf">0.39</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.64</span><span class="p">,</span>  <span class="mf">0.97</span><span class="p">,</span>  <span class="mf">0.82</span><span class="p">,</span>  <span class="mf">0.06</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.41</span><span class="p">,</span>  <span class="mf">0.03</span><span class="p">,</span>  <span class="mf">0.89</span><span class="p">,</span>  <span class="mf">0.24</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.88</span><span class="p">,</span>  <span class="mf">0.29</span><span class="p">,</span>  <span class="mf">0.15</span><span class="p">,</span>  <span class="mf">0.09</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">0.13</span><span class="p">,</span>  <span class="mf">0.19</span><span class="p">,</span>  <span class="mf">0.17</span><span class="p">,</span>  <span class="mf">0.19</span><span class="p">]])</span>
</code></pre></div></div> <p>This generates our initial population of 10 random vectors. Each component <code class="language-plaintext highlighter-rouge">x[i]</code> is normalized between [0, 1]. We will use the bounds to denormalize each component only for evaluating them with <strong>fobj</strong>.</p> <h2 id="evaluation">Evaluation</h2> <p>The next step is to apply a linear transformation to convert each component from [0, 1] to [min, max]. This is only required to evaluate each vector with the function <strong>fobj</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">pop_denorm</span> <span class="o">=</span> <span class="n">min_b</span> <span class="o">+</span> <span class="n">pop</span> <span class="o">*</span> <span class="n">diff</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pop_denorm</span>

<span class="nf">array</span><span class="p">([[</span><span class="o">-</span><span class="mf">4.06</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.89</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span>  <span class="p">,</span> <span class="o">-</span><span class="mf">2.87</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">4.57</span><span class="p">,</span>  <span class="mf">3.69</span><span class="p">,</span>  <span class="mf">0.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.95</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">4.58</span><span class="p">,</span>  <span class="mf">2.78</span><span class="p">,</span>  <span class="mf">1.51</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.31</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">2.83</span><span class="p">,</span>  <span class="mf">3.27</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.72</span><span class="p">,</span>  <span class="mf">3.43</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">3.</span>  <span class="p">,</span> <span class="o">-</span><span class="mf">0.68</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.43</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.57</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">4.47</span><span class="p">,</span>  <span class="mf">4.92</span><span class="p">,</span>  <span class="mf">4.27</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.05</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">1.36</span><span class="p">,</span>  <span class="mf">4.74</span><span class="p">,</span>  <span class="mf">3.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.37</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">0.89</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.67</span><span class="p">,</span>  <span class="mf">3.85</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.61</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">3.76</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.53</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.06</span><span class="p">],</span>
       <span class="p">[</span><span class="o">-</span><span class="mf">3.67</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.14</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.34</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.06</span><span class="p">]])</span>
</code></pre></div></div> <p>At this point we have our initial population of 10 vectors, and now we can evaluate them using our <strong>fobj</strong>. Although these vectors are random points of the function space, some of them are better than others (have a lower \(f(x)\)). Let’s evaluate them:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">pop_denorm</span><span class="p">:</span>
<span class="o">&gt;&gt;&gt;</span>    <span class="nf">print</span><span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="nf">fobj</span><span class="p">(</span><span class="n">ind</span><span class="p">))</span>

<span class="p">[</span><span class="o">-</span><span class="mf">4.06</span> <span class="o">-</span><span class="mf">4.89</span> <span class="o">-</span><span class="mf">1.</span>   <span class="o">-</span><span class="mf">2.87</span><span class="p">]</span> <span class="mf">12.3984504837</span>
<span class="p">[</span><span class="o">-</span><span class="mf">4.57</span>  <span class="mf">3.69</span>  <span class="mf">0.19</span> <span class="o">-</span><span class="mf">4.95</span><span class="p">]</span> <span class="mf">14.7767132816</span>
<span class="p">[</span> <span class="mf">4.58</span>  <span class="mf">2.78</span>  <span class="mf">1.51</span> <span class="o">-</span><span class="mf">3.31</span><span class="p">]</span> <span class="mf">10.4889711137</span>
<span class="p">[</span><span class="o">-</span><span class="mf">2.83</span>  <span class="mf">3.27</span> <span class="o">-</span><span class="mf">2.72</span>  <span class="mf">3.43</span><span class="p">]</span> <span class="mf">9.44800715266</span>
<span class="p">[</span> <span class="mf">3.</span>   <span class="o">-</span><span class="mf">0.68</span> <span class="o">-</span><span class="mf">4.43</span> <span class="o">-</span><span class="mf">0.57</span><span class="p">]</span> <span class="mf">7.34888318457</span>
<span class="p">[</span> <span class="mf">4.47</span>  <span class="mf">4.92</span>  <span class="mf">4.27</span> <span class="o">-</span><span class="mf">1.05</span><span class="p">]</span> <span class="mf">15.8691538075</span>
<span class="p">[</span> <span class="mf">1.36</span>  <span class="mf">4.74</span>  <span class="mf">3.19</span> <span class="o">-</span><span class="mf">4.37</span><span class="p">]</span> <span class="mf">13.4024093959</span>
<span class="p">[</span><span class="o">-</span><span class="mf">0.89</span> <span class="o">-</span><span class="mf">4.67</span>  <span class="mf">3.85</span> <span class="o">-</span><span class="mf">2.61</span><span class="p">]</span> <span class="mf">11.0571791104</span>
<span class="p">[</span> <span class="mf">3.76</span> <span class="o">-</span><span class="mf">2.14</span> <span class="o">-</span><span class="mf">3.53</span> <span class="o">-</span><span class="mf">4.06</span><span class="p">]</span> <span class="mf">11.9129095178</span>
<span class="p">[</span><span class="o">-</span><span class="mf">3.67</span> <span class="o">-</span><span class="mf">3.14</span> <span class="o">-</span><span class="mf">3.34</span> <span class="o">-</span><span class="mf">3.06</span><span class="p">]</span> <span class="mf">10.9544056745</span>
</code></pre></div></div> <p>After evaluating these random vectors, we can see that the vector <code class="language-plaintext highlighter-rouge">x=[ 3., -0.68, -4.43, -0.57]</code> is the best of the population, with a \(f(x)=7.34\), so these values should be closer to the ones that we’re looking for. The evaluation of this initial population is done in L. 9 and stored in the variable fitness.</p> <h2 id="mutation--recombination">Mutation &amp; Recombination</h2> <p>How can the algorithm find a good solution starting from this set of random values?. This is when the interesting part comes. Now, for each vector <code class="language-plaintext highlighter-rouge">pop[j]</code> in the population (from j=0 to 9), we select three other vectors that are not the current one, let’s call them <em>a</em>, <em>b</em> and <em>c</em>. So we start with the first vector <code class="language-plaintext highlighter-rouge">pop[0] = [-4.06 -4.89 -1. -2.87]</code> (called target vector), and in order to select <em>a</em>, <em>b</em> and <em>c</em>, what I do is first I generate a list with the indexes of the vectors in the population, excluding the current one (j=0) (L. 14):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">target</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="c1"># (j = 0)
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">popsize</span><span class="p">)</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="n">j</span><span class="p">]</span> <span class="c1"># (j = 0)
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">idxs</span>

<span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
</code></pre></div></div> <p>And then I randomly choose 3 indexes without replacement (L. 14-15):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">selected</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">idxs</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">selected</span>

<span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
</code></pre></div></div> <p>Here are our candidates (taken from the normalized population):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">pop</span><span class="p">[</span><span class="n">selected</span><span class="p">]</span>

<span class="nf">array</span><span class="p">([[</span> <span class="mf">0.04</span><span class="p">,</span>  <span class="mf">0.87</span><span class="p">,</span>  <span class="mf">0.52</span><span class="p">,</span>  <span class="mf">0.</span>  <span class="p">],</span>  <span class="c1"># a
</span>       <span class="p">[</span> <span class="mf">0.8</span> <span class="p">,</span>  <span class="mf">0.43</span><span class="p">,</span>  <span class="mf">0.06</span><span class="p">,</span>  <span class="mf">0.44</span><span class="p">],</span>  <span class="c1"># b
</span>       <span class="p">[</span> <span class="mf">0.41</span><span class="p">,</span>  <span class="mf">0.03</span><span class="p">,</span>  <span class="mf">0.89</span><span class="p">,</span>  <span class="mf">0.24</span><span class="p">]])</span> <span class="c1"># c
</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">selected</span><span class="p">]</span>
</code></pre></div></div> <p>Now, we create a mutant vector by combining <em>a</em>, <em>b</em> and <em>c</em>. How? by computing the <em>difference</em> (now you know why it’s called <em>differential</em> evolution) between <em>b</em> and <em>c</em> and adding those differences to <em>a</em> after multiplying them by a constant called mutation factor (parameter <strong>mut</strong>). A larger mutation factor increases the search radius but may slowdown the convergence of the algorithm. Values for <strong>mut</strong> are usually chosen from the interval [0.5, 2.0]. For this example, we will use the default value of mut = 0.8:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># mut = 0.8
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">mutant</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">mut</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([</span> <span class="mf">0.35</span><span class="p">,</span>  <span class="mf">1.19</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.14</span><span class="p">,</span>  <span class="mf">0.17</span><span class="p">])</span>
</code></pre></div></div> <p>Note that after this operation, we can end up with a vector that is not normalized (the second value is greater than 1 and the third one is smaller than 0). The next step is to fix those situations. There are two common methods: by generating a new random value in the interval [0, 1], or by <em>clipping</em> the number to the interval, so values greater than 1 become 1, and the values smaller than 0 become 0. I chose the second option just because it can be done in one line of code using <code class="language-plaintext highlighter-rouge">numpy.clip</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">mutant</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="nf">array</span><span class="p">([</span> <span class="mf">0.35</span><span class="p">,</span>  <span class="mf">1.</span>  <span class="p">,</span>  <span class="mf">0.</span>  <span class="p">,</span>  <span class="mf">0.17</span><span class="p">])</span>
</code></pre></div></div> <p>Now that we have our mutant vector, the next step to perform is called recombination. Recombination is about mixing the information of the mutant with the information of the current vector to create a trial vector. This is done by changing the numbers at some positions in the current vector with the ones in the mutant vector. For each position, we decide (with some probability defined by <strong>crossp</strong>) if that number will be replaced or not by the one in the mutant at the same position. To generate the crossover points, we just need to generate uniform random values between [0, 1] and check if the values are less than <strong>crossp</strong>. This method is called <em>binomial crossover</em> since the number of selected locations follows a binomial distribution.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">cross_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">crossp</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cross_points</span>

<span class="nf">array</span><span class="p">([</span><span class="bp">False</span><span class="p">,</span>  <span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">,</span>  <span class="bp">True</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
</code></pre></div></div> <p>In this case we obtained two Trues at positions 1 and 3, which means that the values at positions 1 and 3 of the current vector will be taken from the mutant. This can be done in one line again using the numpy function <code class="language-plaintext highlighter-rouge">where</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">trial</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">cross_points</span><span class="p">,</span> <span class="n">mutant</span><span class="p">,</span> <span class="n">pop</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="c1"># j = 0
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">trial</span>

<span class="nf">array</span><span class="p">([</span> <span class="mf">0.09</span><span class="p">,</span>  <span class="mf">1.</span>  <span class="p">,</span>  <span class="mf">0.4</span> <span class="p">,</span>  <span class="mf">0.17</span><span class="p">])</span>

</code></pre></div></div> <h2 id="replacement">Replacement</h2> <p>After generating our new trial vector, we need to denormalize it and evaluate it to measure how good it is. If this mutant is better than the current vector (<code class="language-plaintext highlighter-rouge">pop[0]</code>) then we replace it with the new one.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">trial_denorm</span> <span class="o">=</span> <span class="n">min_b</span> <span class="o">+</span> <span class="n">trial</span> <span class="o">*</span> <span class="n">diff</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">trial_denorm</span>

<span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">4.1</span><span class="p">,</span>  <span class="mf">5.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">1.</span> <span class="p">,</span> <span class="o">-</span><span class="mf">3.3</span><span class="p">])</span>
</code></pre></div></div> <p>And now, we can evaluate this new vector with <strong>fobj</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="nf">fobj</span><span class="p">(</span><span class="n">mutant_denorm</span><span class="p">)</span>

<span class="mf">13.425000000000001</span>
</code></pre></div></div> <p>In this case, the trial vector is worse than the target vector (13.425 &gt; 12.398), so the target vector is preserved and the trial vector discarded. All these steps have to be repeated again for the remaining individuals (pop[j] for j=1 to j=9), which completes the first iteration of the algorithm. After this process, some of the original vectors of the population will be replaced by better ones, and after many iterations, the whole population will eventually converge towards the solution (it’s a kind of magic uh?).</p> <h1 id="polynomial-curve-fitting-example">Polynomial curve fitting example</h1> <p>Let’s see now the algorithm in action with another concrete example. Given a set of points (x, y), the goal of the curve fitting problem is to find the polynomial that better fits the given points by minimizing for example the sum of the distances between each point and the curve. For this purpose, we are going to generate our set of observations (x, y) using the function \(f(x)=cos(x)\), and adding a small amount of gaussian noise:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">cos(x)</span><span class="sh">'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/curve.png"> <div class="caption"> <b>Figure 5.</b> Dataset of 2D points (x, y) generated using the function \(y=cos(x)\) with gaussian noise. </div> </div> </center> <p>Our goal is to fit a curve (defined by a polynomial) to the set of points that we generated before. This curve should be close to the original \(f(x)=cos(x)\) used to generate the points.</p> <p>We would need a polynomial with enough degrees to generate at least 4 curves. For this purpose, a polynomial of degree 5 should be enough (you can try with more/less degrees to see what happens):</p> <p>\[f_{model}(\mathbf{w}, x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + w_4 x^4 + w_5 x^5\]</p> <p>This polynomial has 6 parameters \(\mathbf{w}=\{w_1, w_2, w_3, w_4, w_5, w_6\}\). Different values for those parameters generate different curves. Let’s implement it:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">4</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">5</span>
</code></pre></div></div> <p>Using this expression, we can generate an infinite set of possible curves. For example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nf">fmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.01</span><span class="p">]))</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/curve-example.png"> <div class="caption"> <b>Figure 6.</b> Example of a polynomial of degree 5. </div> </div> </center> <p>Among this infinite set of curves, we want the one that better approximates the original function \(f(x)=cos(x)\). For this purpose, we need a function that measures how good a polynomial is. We can use for example the <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation" rel="external nofollow noopener" target="_blank">Root Mean Square Error (RMSE)</a> function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">fmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div> <p>Now we have a clear description of our problem: we need to find the parameters \(\mathbf{w}=\{w_1, w_2, w_3, w_4, w_5, w_6\}\) for our polynomial of degree 5 that minimizes the rmse function. Let’s evolve a population of 20 random polynomials for 2,000 iterations with DE:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">result</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de</span><span class="p">(</span><span class="n">rmse</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="n">its</span><span class="o">=</span><span class="mi">2000</span><span class="p">))</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">result</span>

<span class="p">(</span><span class="nf">array</span><span class="p">([</span> <span class="mf">0.99677643</span><span class="p">,</span>  <span class="mf">0.47572443</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.39088333</span><span class="p">,</span>  <span class="mf">0.50950016</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06498931</span><span class="p">,</span>
         <span class="mf">0.00273167</span><span class="p">]),</span> <span class="mf">0.214860061914732</span><span class="p">)</span>
</code></pre></div></div> <p>We obtained a solution with a <em>rmse</em> of ~0.215. We can plot this polynomial to see how good our approximation is:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">cos(x)</span><span class="sh">'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nf">fmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.99677643</span><span class="p">,</span> <span class="mf">0.47572443</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.39088333</span><span class="p">,</span> 
<span class="o">&gt;&gt;&gt;</span>                        <span class="mf">0.50950016</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.06498931</span><span class="p">,</span> <span class="mf">0.00273167</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">result</span><span class="sh">'</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div> <center> <div class="l-body"> <img src="/assets/img/de/approximation.png"> <div class="caption"> <b>Figure 7.</b> Approximation of the original function \(f(x)=cos(x)\) used to generate the data points, after 2000 iterations with DE. </div> </div> </center> <p>Not bad at all!. Now let’s see in action how the algorithm evolve the population of random vectors until all of them converge towards the solution. It is very easy to create an animation with matplotlib, using a slight modification of our original DE implementation to yield the entire population after each iteration instead of just the best vector:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Replace this line in DE (L. 29)
</span><span class="k">yield</span> <span class="n">best</span><span class="p">,</span> <span class="n">fitness</span><span class="p">[</span><span class="n">best_idx</span><span class="p">]</span>
<span class="c1"># With this line (and call the new version de2)
</span><span class="k">yield</span> <span class="n">min_b</span> <span class="o">+</span> <span class="n">pop</span> <span class="o">*</span> <span class="n">diff</span><span class="p">,</span> <span class="n">fitness</span><span class="p">,</span> <span class="n">best_idx</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.animation</span> <span class="k">as</span> <span class="n">animation</span>
<span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">result</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">de2</span><span class="p">(</span><span class="n">rmse</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)]</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="n">its</span><span class="o">=</span><span class="mi">2000</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">clear</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">pop</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="n">pop</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="nf">fmodel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ind</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</code></pre></div></div> <p>Now we only need to generate the animation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">anim</span> <span class="o">=</span> <span class="n">animation</span><span class="p">.</span><span class="nc">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">frames</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nc">HTML</span><span class="p">(</span><span class="n">anim</span><span class="p">.</span><span class="nf">to_html5_video</span><span class="p">())</span>
</code></pre></div></div> <center> <figure> <video src="/assets/img/de/curve-fitting.mp4" class="img-fluid" width="auto" height="auto" autoplay="" loop=""></video> </figure> </center> <p>The animation shows how the different vectors in the population (each one corresponding to a different curve) converge towards the solution after a few iterations.</p> <h1 id="de-variations">DE variations</h1> <p>The schema used in this version of the algorithm is called <em>rand/1/bin</em> because the vectors are randomly chosen (<em>rand</em>), we only used <em>1</em> vector difference and the crossover strategy used to mix the information of the trial and the target vectors was a binomial crossover. But there are other variants:</p> <h2 id="mutation-schemas">Mutation schemas</h2> <ul> <li> <strong>Rand/1</strong>: \(x_{mut} = x_{r1} + F(x_{r2} - x_{r3})\)</li> <li> <strong>Rand/2</strong>: \(x_{mut} = x_{r1} + F(x_{r2} - x_{r3} + x_{r4} - x_{r5})\)</li> <li> <strong>Best/1</strong>: \(x_{mut} = x_{best} + F(x_{r2} - x_{r3})\)</li> <li> <strong>Best/2</strong>: \(x_{mut} = x_{best} + F(x_{r2} - x_{r3} + x_{r4} - x_{r5})\)</li> <li> <strong>Rand-to-best/1</strong>: \(x_{mut} = x_{r1} + F_1(x_{r2} - x_{r3}) + F_2(x_{best} - x_{r1})\)</li> <li>…</li> </ul> <h2 id="crossover-schemas">Crossover schemas</h2> <ul> <li> <strong>Binomial (bin)</strong>: crossover due to independent binomial experiments. Each component of the target vector has a probability <em>p</em> of being changed by the component of the the mutant vector.</li> <li> <strong>Exponential (exp)</strong>: it’s a two-point crossover operator, where two locations of the vector are randomly chosen so that <em>n</em> consecutive numbers of the vector (between the two locations) are taken from the mutant vector.</li> </ul> <p>Mutation/crossover schemas can be combined to generate different DE variants, such as <em>rand/2/exp</em>, <em>best/1/exp</em>, <em>rand/2/bin</em> and so on. There is no single strategy “to rule them all”. Some schemas work better on some problems and worse in others. The tricky part is choosing the best variant and the best parameters (mutation factor, crossover probability, population size) for the problem we are trying to solve.</p> <h1 id="final-words">Final words</h1> <p>Differential Evolution (DE) is a very simple but powerful algorithm for optimization of complex functions that works pretty well in those problems where other techniques (such as Gradient Descent) cannot be used. In this post, we’ve seen how to implement it in just 27 lines of Python with Numpy, and we’ve seen how the algorithm works step by step.</p> <p>If you are looking for a Python library for black-box optimization that includes the Differential Evolution algorithm, here are some:</p> <ul> <li> <p><a href="https://github.com/pablormier/yabox" rel="external nofollow noopener" target="_blank">Yabox</a>. Yet another black-box optimization library for Python 3+. This is a project I’ve started recently, and it’s the library I’ve used to generate the figures you’ve seen in this post. Yabox is a very lightweight library that depends only on Numpy.</p> </li> <li> <p><a href="http://esa.github.io/pygmo/" rel="external nofollow noopener" target="_blank">Pygmo</a>. A powerful library for numerical optimization, developed and mantained by the ESA. Pygmo is a scientific library providing a large number of optimization problems and algorithms under the same powerful parallelization abstraction built around the generalized island-model paradigm.</p> </li> <li> <p><a href="https://github.com/Project-Platypus/Platypus" rel="external nofollow noopener" target="_blank">Platypus</a>. Platypus is a framework for evolutionary computing in Python with a focus on multiobjective evolutionary algorithms (MOEAs). It differs from existing optimization libraries, including PyGMO, Inspyred, DEAP, and Scipy, by providing optimization algorithms and analysis tools for multiobjective optimization</p> </li> <li> <p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.differential_evolution.html" rel="external nofollow noopener" target="_blank">Scipy</a>. The well known scientific library for Python includes a fast implementation of the Differential Evolution algorithm.</p> </li> </ul> </div> </article> <div id="disqus_thread" style="max-width: 800px; margin: 0 auto;"> <script type="text/javascript">var disqus_shortname="pablormier",disqus_identifier="/2017/09/05/a-tutorial-on-differential-evolution-with-python",disqus_title="A tutorial on Differential Evolution with Python";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();</script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Pablo Rodriguez Mier. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-2MZCXEWC72"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-2MZCXEWC72");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>